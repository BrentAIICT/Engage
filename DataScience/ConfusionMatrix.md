# Confusion Matrix and Model Performance Metrics

## Confusion Matrix

A confusion matrix is a table used to describe the performance of a classification model on a set of test data for which the true values are known (supervised learning).

Matrix Definitions:

* TP: True Positives - Predicted the result to be true and it was true.
* TN: True Negatives - Predicted the result to be false and it was false.
* FP: False Positives - Predicted the result to be true and it was false.
* FN: False Negatives - Predicted the result to be false and it was true.

|n = 140| Actual<br/>True | Actual<br/>False |
|-|-|-|
| Predicted<br/>True | TP: 50 | FP: 20 |
| Predicted<br/> False | FN: 40 | TN: 30 |

* Accurace: 0.57
* Error Rate: 
* Percision:
* F1 Score:
* AUC: 

## Accuracy

How often is the classifer correct?

```
Accuracy = (TP + TN) / Total

For the above example: ( 50 + 30 ) / 140 = 0.57
```
## Error Rate

How often is the classifier incorrect?

```
Error Rate = (FP + FN) / Total
```


## Precision

## Recall

## F1 Score

## AUC



