# Confusion Matrix and Model Performance Metrics

## Confusion Matrix

A confusion matrix is a table used to describe the performance of a classification model on a set of test data for which the true values are known (supervised learning).

|n = 140| Actual True | Actual False |
|-|-|-|
| Predicted True | True Positives: 50 | False Positives: 20 |
| Predicted False | False Negatives: 40 | True Negatives: 30 |

* Accurace: 0.57
* Percision:
* F1 Score:
* AUC: 

## Accuracy

Overall, how often is the classifer correct?

```
Accuracy = (True Positives + True Negatives) / Total Classified

For the above example: ( 50 + 30 ) / 140 = 0.57
```

## Precision

## Recall

## F1 Score

## AUC



